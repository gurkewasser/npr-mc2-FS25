{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if static seed is needed for reproducibility\n",
    "\n",
    "## Baseline (Log Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Size |     Metric |   Original |     Diff_2 |     Diff_3 |     Diff_4\n",
      "-----------------------------------------------------------------------\n",
      "    25 | accuracy   |     0.5012 |     0.0000 |     0.0000 |     0.0000\n",
      "    25 | precision  |     1.0000 |     0.0000 |     0.0000 |     0.0000\n",
      "    25 | recall     |     0.0025 |     0.0000 |     0.0000 |     0.0000\n",
      "    25 | f1         |     0.0050 |     0.0000 |     0.0000 |     0.0000\n",
      "    50 | accuracy   |     0.5800 |     0.0000 |     0.0000 |     0.0000\n",
      "    50 | precision  |     0.5478 |     0.0000 |     0.0000 |     0.0000\n",
      "    50 | recall     |     0.9175 |     0.0000 |     0.0000 |     0.0000\n",
      "    50 | f1         |     0.6860 |     0.0000 |     0.0000 |     0.0000\n",
      "   100 | accuracy   |     0.6737 |     0.0000 |     0.0000 |     0.0000\n",
      "   100 | precision  |     0.6248 |     0.0000 |     0.0000 |     0.0000\n",
      "   100 | recall     |     0.8700 |     0.0000 |     0.0000 |     0.0000\n",
      "   100 | f1         |     0.7273 |     0.0000 |     0.0000 |     0.0000\n",
      "   150 | accuracy   |     0.7037 |     0.0000 |     0.0000 |     0.0000\n",
      "   150 | precision  |     0.7245 |     0.0000 |     0.0000 |     0.0000\n",
      "   150 | recall     |     0.6575 |     0.0000 |     0.0000 |     0.0000\n",
      "   150 | f1         |     0.6894 |     0.0000 |     0.0000 |     0.0000\n",
      "   200 | accuracy   |     0.7500 |     0.0000 |     0.0000 |     0.0000\n",
      "   200 | precision  |     0.7242 |     0.0000 |     0.0000 |     0.0000\n",
      "   200 | recall     |     0.8075 |     0.0000 |     0.0000 |     0.0000\n",
      "   200 | f1         |     0.7636 |     0.0000 |     0.0000 |     0.0000\n",
      "   250 | accuracy   |     0.7588 |     0.0000 |     0.0000 |     0.0000\n",
      "   250 | precision  |     0.8371 |     0.0000 |     0.0000 |     0.0000\n",
      "   250 | recall     |     0.6425 |     0.0000 |     0.0000 |     0.0000\n",
      "   250 | f1         |     0.7270 |     0.0000 |     0.0000 |     0.0000\n",
      "   300 | accuracy   |     0.7712 |     0.0000 |     0.0000 |     0.0000\n",
      "   300 | precision  |     0.8444 |     0.0000 |     0.0000 |     0.0000\n",
      "   300 | recall     |     0.6650 |     0.0000 |     0.0000 |     0.0000\n",
      "   300 | f1         |     0.7441 |     0.0000 |     0.0000 |     0.0000\n",
      "   350 | accuracy   |     0.7975 |     0.0000 |     0.0000 |     0.0000\n",
      "   350 | precision  |     0.8306 |     0.0000 |     0.0000 |     0.0000\n",
      "   350 | recall     |     0.7475 |     0.0000 |     0.0000 |     0.0000\n",
      "   350 | f1         |     0.7868 |     0.0000 |     0.0000 |     0.0000\n",
      "   400 | accuracy   |     0.7963 |     0.0000 |     0.0000 |     0.0000\n",
      "   400 | precision  |     0.8015 |     0.0000 |     0.0000 |     0.0000\n",
      "   400 | recall     |     0.7875 |     0.0000 |     0.0000 |     0.0000\n",
      "   400 | f1         |     0.7945 |     0.0000 |     0.0000 |     0.0000\n",
      "\n",
      "=== Difference Test Summary ===\n",
      "Metric 'accuracy': No significant differences across runs (all diffs <= 1e-06)\n",
      "Metric 'precision': No significant differences across runs (all diffs <= 1e-06)\n",
      "Metric 'recall': No significant differences across runs (all diffs <= 1e-06)\n",
      "Metric 'f1': No significant differences across runs (all diffs <= 1e-06)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directory containing the results\n",
    "results_dir = \"../results/classic\"\n",
    "\n",
    "# Steps to compare\n",
    "sizes = [25, 50, 100, 150, 200, 250, 300, 350, 400]\n",
    "\n",
    "# Metrics to compare\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "\n",
    "header = (\n",
    "    f\"{'Size':>6} | {'Metric':>10} | {'Original':>10} | {'Diff_2':>10} | {'Diff_3':>10} | {'Diff_4':>10}\"\n",
    ")\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "# Collect all differences for a small test at the end\n",
    "all_differences = {metric: [] for metric in metrics}\n",
    "\n",
    "for size in sizes:\n",
    "    file1 = os.path.join(results_dir, f\"classical_results_{size}.csv\")\n",
    "    file2 = os.path.join(results_dir, f\"classical_results_{size}_2.csv\")\n",
    "    file3 = os.path.join(results_dir, f\"classical_results_{size}_3.csv\")\n",
    "    file4 = os.path.join(results_dir, f\"classical_results_{size}_4.csv\")\n",
    "    missing = []\n",
    "    for f in [file1, file2, file3, file4]:\n",
    "        if not os.path.exists(f):\n",
    "            missing.append(f)\n",
    "    if missing:\n",
    "        print(f\"Missing file(s) for size {size}: {' '.join(missing)}\")\n",
    "        continue\n",
    "\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "    df3 = pd.read_csv(file3)\n",
    "    df4 = pd.read_csv(file4)\n",
    "\n",
    "    # Assume single row per file\n",
    "    row1 = df1.iloc[0]\n",
    "    row2 = df2.iloc[0]\n",
    "    row3 = df3.iloc[0]\n",
    "    row4 = df4.iloc[0]\n",
    "\n",
    "    for metric in metrics:\n",
    "        val1 = row1[metric] if metric in row1 else float('nan')\n",
    "        val2 = row2[metric] if metric in row2 else float('nan')\n",
    "        val3 = row3[metric] if metric in row3 else float('nan')\n",
    "        val4 = row4[metric] if metric in row4 else float('nan')\n",
    "        diff2 = val2 - val1\n",
    "        diff3 = val3 - val1\n",
    "        diff4 = val4 - val1\n",
    "        print(\n",
    "            f\"{size:6} | {metric:10} | {val1:10.4f} | {diff2:10.4f} | {diff3:10.4f} | {diff4:10.4f}\"\n",
    "        )\n",
    "        # Collect differences for the test\n",
    "        all_differences[metric].extend([diff2, diff3, diff4])\n",
    "\n",
    "# === Difference Test Summary ===\n",
    "tolerance = 1e-6\n",
    "print(\"\\n=== Difference Test Summary ===\")\n",
    "for metric in metrics:\n",
    "    diffs = all_differences[metric]\n",
    "    # Check if any difference is greater than tolerance\n",
    "    significant_diffs = [d for d in diffs if abs(d) > tolerance]\n",
    "    if significant_diffs:\n",
    "        print(f\"Metric '{metric}': Differences found in {len(significant_diffs)} run(s) (max diff: {max(abs(d) for d in significant_diffs):.6f})\")\n",
    "    else:\n",
    "        print(f\"Metric '{metric}': No significant differences across runs (all diffs <= {tolerance})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Baseline (LogReg)\n",
    "\n",
    "In our classical baseline using TF-IDF vectorization and logistic regression, we ran the training pipeline **four times** and compared the resulting metrics across different training sizes.\n",
    "\n",
    "The results showed **no variation across runs** in any of the evaluated metrics (`accuracy`, `precision`, `recall`, `f1`) for each training size. Differences in subsequent runs (`Diff_2`, `Diff_3`, `Diff_4`) were either **zero** or **extremely small (≤ 0.0012)**—values that can be attributed to numerical rounding rather than stochastic behavior.\n",
    "\n",
    "This consistency confirms that:\n",
    "- The TF-IDF vectorization with fixed vocabulary size (`max_features=5000`) and n-gram range is deterministic.\n",
    "- Logistic Regression (with `max_iter=1000`) behaves deterministically on the given input.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Size | Epoch |     Metric |   Original |     Diff_2 |     Diff_3 |     Diff_4\n",
      "-------------------------------------------------------------------------------\n",
      "    25 |    50 | eval_accuracy |     0.7575 |    -0.0675 |    -0.0900 |    -0.0687\n",
      "    25 |    50 | eval_precision |     0.8301 |     0.0062 |    -0.0170 |    -0.0452\n",
      "    25 |    50 | eval_recall |     0.6475 |    -0.1750 |    -0.2125 |    -0.1275\n",
      "    25 |    50 | eval_f1    |     0.7275 |    -0.1237 |    -0.1608 |    -0.1020\n",
      "    50 |    50 | eval_accuracy |     0.7762 |     0.0038 |    -0.0100 |     0.0000\n",
      "    50 |    50 | eval_precision |     0.7826 |    -0.0233 |    -0.0222 |     0.0000\n",
      "    50 |    50 | eval_recall |     0.7650 |     0.0550 |     0.0125 |     0.0000\n",
      "    50 |    50 | eval_f1    |     0.7737 |     0.0148 |    -0.0049 |     0.0000\n",
      "   100 |    50 | eval_accuracy |     0.8662 |     0.0025 |     0.0175 |     0.0112\n",
      "   100 |    50 | eval_precision |     0.8886 |    -0.0297 |     0.0229 |     0.0109\n",
      "   100 |    50 | eval_recall |     0.8375 |     0.0450 |     0.0125 |     0.0125\n",
      "   100 |    50 | eval_f1    |     0.8623 |     0.0082 |     0.0174 |     0.0117\n",
      "   150 |    50 | eval_accuracy |     0.8938 |    -0.0138 |     0.0050 |     0.0025\n",
      "   150 |    50 | eval_precision |     0.9156 |    -0.0298 |     0.0031 |    -0.0183\n",
      "   150 |    50 | eval_recall |     0.8675 |     0.0050 |     0.0075 |     0.0275\n",
      "   150 |    50 | eval_f1    |     0.8909 |    -0.0118 |     0.0054 |     0.0052\n",
      "   200 |    50 | eval_accuracy |     0.9100 |    -0.0013 |    -0.0038 |    -0.0038\n",
      "   200 |    50 | eval_precision |     0.9184 |     0.0371 |     0.0059 |     0.0104\n",
      "   200 |    50 | eval_recall |     0.9000 |    -0.0425 |    -0.0150 |    -0.0200\n",
      "   200 |    50 | eval_f1    |     0.9091 |    -0.0053 |    -0.0049 |    -0.0054\n",
      "   250 |    50 | eval_accuracy |     0.9062 |     0.0075 |     0.0025 |     0.0000\n",
      "   250 |    50 | eval_precision |     0.9114 |     0.0445 |     0.0390 |     0.0000\n",
      "   250 |    50 | eval_recall |     0.9000 |    -0.0325 |    -0.0375 |     0.0000\n",
      "   250 |    50 | eval_f1    |     0.9057 |     0.0039 |    -0.0013 |     0.0000\n",
      "   300 |    50 | eval_accuracy |     0.9125 |    -0.0050 |    -0.0012 |     0.0000\n",
      "   300 |    50 | eval_precision |     0.9297 |    -0.0074 |    -0.0046 |     0.0000\n",
      "   300 |    50 | eval_recall |     0.8925 |    -0.0025 |     0.0025 |     0.0000\n",
      "   300 |    50 | eval_f1    |     0.9107 |    -0.0049 |    -0.0009 |     0.0000\n",
      "   350 |    50 | eval_accuracy |     0.9087 |     0.0013 |     0.0050 |     0.0000\n",
      "   350 |    50 | eval_precision |     0.9314 |     0.0025 |    -0.0207 |     0.0000\n",
      "   350 |    50 | eval_recall |     0.8825 |     0.0000 |     0.0350 |     0.0000\n",
      "   350 |    50 | eval_f1    |     0.9063 |     0.0012 |     0.0078 |     0.0000\n",
      "   400 |    50 | eval_accuracy |     0.9137 |    -0.0025 |    -0.0025 |     0.0000\n",
      "   400 |    50 | eval_precision |     0.9276 |    -0.0048 |    -0.0215 |     0.0000\n",
      "   400 |    50 | eval_recall |     0.8975 |     0.0000 |     0.0200 |     0.0000\n",
      "   400 |    50 | eval_f1    |     0.9123 |    -0.0023 |    -0.0005 |     0.0000\n",
      "\n",
      "=== Difference Test Summary ===\n",
      "Metric 'eval_accuracy': Differences found in 22 run(s) (max diff: 0.090000)\n",
      "Metric 'eval_precision': Differences found in 22 run(s) (max diff: 0.045223)\n",
      "Metric 'eval_recall': Differences found in 20 run(s) (max diff: 0.212500)\n",
      "Metric 'eval_f1': Differences found in 22 run(s) (max diff: 0.160753)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directory containing the results\n",
    "results_dir = \"../results/fine-tuning\"\n",
    "\n",
    "sizes = [25, 50, 100, 150, 200, 250, 300, 350, 400]\n",
    "epoch = 50  # always use 50 epochs\n",
    "\n",
    "# Metrics to compare\n",
    "metrics = [\"eval_accuracy\", \"eval_precision\", \"eval_recall\", \"eval_f1\"]  # adjust as needed\n",
    "\n",
    "header = (\n",
    "    f\"{'Size':>6} | {'Epoch':>5} | {'Metric':>10} | {'Original':>10} | {'Diff_2':>10} | {'Diff_3':>10} | {'Diff_4':>10}\"\n",
    ")\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "# To collect all differences for the test at the end\n",
    "all_differences = {metric: [] for metric in metrics}\n",
    "\n",
    "for size in sizes:\n",
    "    # Build file names\n",
    "    base = f\"fine_tuning_metrics_{size}_{epoch}e\"\n",
    "    files = [\n",
    "        os.path.join(results_dir, f\"{base}.csv\"),\n",
    "        os.path.join(results_dir, f\"{base}_2.csv\"),\n",
    "        os.path.join(results_dir, f\"{base}_3.csv\"),\n",
    "        os.path.join(results_dir, f\"{base}_4.csv\"),\n",
    "    ]\n",
    "    missing = [f for f in files if not os.path.exists(f)]\n",
    "    if missing:\n",
    "        print(f\"Missing file(s) for size {size}, epoch {epoch}: {' '.join(os.path.basename(m) for m in missing)}\")\n",
    "        continue\n",
    "\n",
    "    # Read the files\n",
    "    dfs = [pd.read_csv(f) for f in files]\n",
    "    # Assume single row per file\n",
    "    rows = [df.iloc[0] for df in dfs]\n",
    "\n",
    "    for metric in metrics:\n",
    "        vals = [row[metric] if metric in row else float('nan') for row in rows]\n",
    "        diff2 = vals[1] - vals[0]\n",
    "        diff3 = vals[2] - vals[0]\n",
    "        diff4 = vals[3] - vals[0]\n",
    "        print(\n",
    "            f\"{size:6} | {epoch:5} | {metric:10} | {vals[0]:10.4f} | {diff2:10.4f} | {diff3:10.4f} | {diff4:10.4f}\"\n",
    "        )\n",
    "        # Collect differences for the test\n",
    "        all_differences[metric].extend([diff2, diff3, diff4])\n",
    "\n",
    "# Test at the end: check if there are any nonzero differences (beyond a small tolerance)\n",
    "tolerance = 1e-6\n",
    "print(\"\\n=== Difference Test Summary ===\")\n",
    "for metric in metrics:\n",
    "    diffs = all_differences[metric]\n",
    "    # Check if any difference is greater than tolerance\n",
    "    significant_diffs = [d for d in diffs if abs(d) > tolerance]\n",
    "    if significant_diffs:\n",
    "        print(f\"Metric '{metric}': Differences found in {len(significant_diffs)} run(s) (max diff: {max(abs(d) for d in significant_diffs):.6f})\")\n",
    "    else:\n",
    "        print(f\"Metric '{metric}': No significant differences across runs (all diffs <= {tolerance})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Fine-tuning results\n",
    "\n",
    "Based on the evaluation results across different training sizes and runs, there is clear evidence of performance variability despite identical training configurations. For example:\n",
    "\n",
    "- `eval_accuracy` differences reach up to **0.09**\n",
    "- `eval_recall` differences go as high as **0.21**\n",
    "- Similar fluctuations are observed in `precision` and `f1` scores\n",
    "\n",
    "These variations indicate that randomness in model initialization, data shuffling, or other stochastic elements significantly impacts the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Size | Epoch |     Metric |   Original |     Diff_2 |     Diff_3 |     Diff_4\n",
      "-------------------------------------------------------------------------------\n",
      "    25 |    50 | eval_accuracy |     0.5038 |     0.0137 |     0.0037 |     0.0037\n",
      "    25 |    50 | eval_precision |     0.5021 |     0.0216 |     0.0579 |     0.0579\n",
      "    25 |    50 | eval_recall |     0.9000 |    -0.5125 |    -0.8300 |    -0.8300\n",
      "    25 |    50 | eval_f1    |     0.6446 |    -0.1992 |    -0.5201 |    -0.5201\n",
      "    50 |    50 | eval_accuracy |     0.5363 |     0.0000 |    -0.0175 |    -0.0175\n",
      "    50 |    50 | eval_precision |     0.5502 |     0.0000 |     0.0016 |     0.0016\n",
      "    50 |    50 | eval_recall |     0.3975 |     0.0000 |    -0.1975 |    -0.1975\n",
      "    50 |    50 | eval_f1    |     0.4615 |     0.0000 |    -0.1680 |    -0.1680\n",
      "   100 |    50 | eval_accuracy |     0.4863 |     0.0175 |     0.0275 |     0.0125\n",
      "   100 |    50 | eval_precision |     0.4883 |     0.0136 |     0.0224 |     0.0103\n",
      "   100 |    50 | eval_recall |     0.5750 |     0.4050 |     0.0775 |    -0.1250\n",
      "   100 |    50 | eval_f1    |     0.5281 |     0.1357 |     0.0449 |    -0.0551\n",
      "   150 |    50 | eval_accuracy |     0.5212 |     0.0225 |    -0.0225 |    -0.0212\n",
      "   150 |    50 | eval_precision |     0.5825 |    -0.0427 |    -0.0953 |    -0.0825\n",
      "   150 |    50 | eval_recall |     0.1500 |     0.4425 |    -0.1025 |     0.6500\n",
      "   150 |    50 | eval_f1    |     0.2386 |     0.3264 |    -0.1520 |     0.3768\n",
      "   200 |    50 | eval_accuracy |     0.5563 |    -0.0550 |    -0.0588 |    -0.0588\n",
      "   200 |    50 | eval_precision |     0.5432 |    -0.0426 |    -0.0987 |    -0.0987\n",
      "   200 |    50 | eval_recall |     0.7075 |     0.2800 |    -0.6875 |    -0.6875\n",
      "   200 |    50 | eval_f1    |     0.6145 |     0.0499 |    -0.5763 |    -0.5763\n",
      "   250 |    50 | eval_accuracy |     0.5413 |     0.0000 |    -0.0125 |    -0.0125\n",
      "   250 |    50 | eval_precision |     0.5514 |     0.0000 |    -0.0129 |    -0.0129\n",
      "   250 |    50 | eval_recall |     0.4425 |     0.0000 |    -0.0400 |    -0.0400\n",
      "   250 |    50 | eval_f1    |     0.4910 |     0.0000 |    -0.0303 |    -0.0303\n",
      "   300 |    50 | eval_accuracy |     0.4988 |     0.0000 |     0.0050 |     0.0050\n",
      "   300 |    50 | eval_precision |     0.4762 |     0.0000 |     0.5238 |     0.5238\n",
      "   300 |    50 | eval_recall |     0.0250 |     0.0000 |    -0.0175 |    -0.0175\n",
      "   300 |    50 | eval_f1    |     0.0475 |     0.0000 |    -0.0326 |    -0.0326\n",
      "   350 |    50 | eval_accuracy |     0.4938 |     0.0000 |     0.0700 |     0.0700\n",
      "   350 |    50 | eval_precision |     0.4074 |     0.0000 |     0.1701 |     0.1701\n",
      "   350 |    50 | eval_recall |     0.0275 |     0.0000 |     0.4475 |     0.4475\n",
      "   350 |    50 | eval_f1    |     0.0515 |     0.0000 |     0.4697 |     0.4697\n",
      "   400 |    50 | eval_accuracy |     0.6025 |     0.0000 |    -0.0162 |    -0.0162\n",
      "   400 |    50 | eval_precision |     0.6085 |     0.0000 |    -0.0150 |    -0.0150\n",
      "   400 |    50 | eval_recall |     0.5750 |     0.0000 |    -0.0275 |    -0.0275\n",
      "   400 |    50 | eval_f1    |     0.5913 |     0.0000 |    -0.0217 |    -0.0217\n",
      "\n",
      "=== Difference Test Summary ===\n",
      "Metric 'eval_accuracy': Differences found in 22 run(s) (max diff: 0.070000)\n",
      "Metric 'eval_precision': Differences found in 22 run(s) (max diff: 0.523810)\n",
      "Metric 'eval_recall': Differences found in 22 run(s) (max diff: 0.830000)\n",
      "Metric 'eval_f1': Differences found in 22 run(s) (max diff: 0.576272)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directory containing the results\n",
    "results_dir = \"../results/transfer\"\n",
    "\n",
    "sizes = [25, 50, 100, 150, 200, 250, 300, 350, 400]\n",
    "epoch = 50  # always use 50 epochs\n",
    "\n",
    "# Metrics to compare\n",
    "metrics = [\"eval_accuracy\", \"eval_precision\", \"eval_recall\", \"eval_f1\"]  # adjust as needed\n",
    "\n",
    "header = (\n",
    "    f\"{'Size':>6} | {'Epoch':>5} | {'Metric':>10} | {'Original':>10} | {'Diff_2':>10} | {'Diff_3':>10} | {'Diff_4':>10}\"\n",
    ")\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "# To collect all differences for the test at the end\n",
    "all_differences = {metric: [] for metric in metrics}\n",
    "\n",
    "for size in sizes:\n",
    "    # Build file names\n",
    "    base = f\"transfer_metrics_{size}_{epoch}e\"\n",
    "    files = [\n",
    "        os.path.join(results_dir, f\"{base}.csv\"),\n",
    "        os.path.join(results_dir, f\"{base}_2.csv\"),\n",
    "        os.path.join(results_dir, f\"{base}_3.csv\"),\n",
    "        os.path.join(results_dir, f\"{base}_4.csv\"),\n",
    "    ]\n",
    "    missing = [f for f in files if not os.path.exists(f)]\n",
    "    if missing:\n",
    "        print(f\"Missing file(s) for size {size}, epoch {epoch}: {' '.join(os.path.basename(m) for m in missing)}\")\n",
    "        continue\n",
    "\n",
    "    # Read the files\n",
    "    dfs = [pd.read_csv(f) for f in files]\n",
    "    # Assume single row per file\n",
    "    rows = [df.iloc[0] for df in dfs]\n",
    "\n",
    "    for metric in metrics:\n",
    "        vals = [row[metric] if metric in row else float('nan') for row in rows]\n",
    "        diff2 = vals[1] - vals[0]\n",
    "        diff3 = vals[2] - vals[0]\n",
    "        diff4 = vals[3] - vals[0]\n",
    "        print(\n",
    "            f\"{size:6} | {epoch:5} | {metric:10} | {vals[0]:10.4f} | {diff2:10.4f} | {diff3:10.4f} | {diff4:10.4f}\"\n",
    "        )\n",
    "        # Collect differences for the test\n",
    "        all_differences[metric].extend([diff2, diff3, diff4])\n",
    "\n",
    "# Test at the end: check if there are any nonzero differences (beyond a small tolerance)\n",
    "tolerance = 1e-6\n",
    "print(\"\\n=== Difference Test Summary ===\")\n",
    "for metric in metrics:\n",
    "    diffs = all_differences[metric]\n",
    "    # Check if any difference is greater than tolerance\n",
    "    significant_diffs = [d for d in diffs if abs(d) > tolerance]\n",
    "    if significant_diffs:\n",
    "        print(f\"Metric '{metric}': Differences found in {len(significant_diffs)} run(s) (max diff: {max(abs(d) for d in significant_diffs):.6f})\")\n",
    "    else:\n",
    "        print(f\"Metric '{metric}': No significant differences across runs (all diffs <= {tolerance})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Transfer-learning\n",
    "\n",
    "The evaluation results clearly show significant variability in model performance across different runs, despite using identical training settings. For instance:\n",
    "\n",
    "- `eval_accuracy` differences reach up to **0.07**\n",
    "- `eval_precision` differences go as high as **0.52**\n",
    "- `eval_recall` fluctuates by as much as **0.83**\n",
    "- `eval_f1` varies by up to **0.58**\n",
    "\n",
    "These fluctuations suggest that stochastic elements such as weight initialization, data shuffling, and batching are introducing instability into the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Why Using a Static Seed Is Important for Training Deep Learning Models\n",
    "\n",
    "In machine learning—especially in deep learning—training is often influenced by stochastic processes such as:\n",
    "\n",
    "- Random weight initialization\n",
    "- Data shuffling during training\n",
    "- Dropout layers and other regularization techniques\n",
    "\n",
    "These sources of randomness can lead to **non-deterministic outcomes**, meaning that running the same training pipeline multiple times may produce significantly different results. This has been observed in both **fine-tuning** and **transfer learning** experiments, where evaluation metrics (like accuracy, precision, recall, and F1 score) vary noticeably across runs.\n",
    "\n",
    "By setting a **static random seed**, we can:\n",
    "\n",
    "- **Ensure reproducibility**: Results can be reliably replicated for debugging or reporting.\n",
    "- **Stabilize model comparison**: Differences in performance between models or configurations can be attributed to meaningful changes rather than random noise.\n",
    "- **Simplify experimentation**: It becomes easier to isolate the effects of hyperparameter tuning or architectural changes.\n",
    "\n",
    "While a static seed isn't necessary for deterministic algorithms like logistic regression (as confirmed by the baseline results), it's **strongly recommended** for any model or pipeline involving randomness.\n",
    "\n",
    "### Setting a Static Seed in PyTorch\n",
    "\n",
    "To enforce reproducibility in PyTorch-based experiments, the following function will be used and is found in `utils.py`:\n",
    "\n",
    "```python\n",
    "SEED = 2277\n",
    "\n",
    "def set_seed():\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npr_mc2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
